{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import fasttext\n",
    "from get_glove_embeddings import glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cc.hi.300.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_154/3778267266.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mwords_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"hi_words_reduced\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhi\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"eng_words_reduced\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mop_wv_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ft.hi.300.txt\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhi\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"ft.en.300.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mwv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_facebook_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfasttext_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mfile_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ss/lib/python3.7/site-packages/gensim/models/fasttext.py\u001b[0m in \u001b[0;36mload_facebook_vectors\u001b[0;34m(path, encoding)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m     \"\"\"\n\u001b[0;32m--> 784\u001b[0;31m     \u001b[0mfull_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_fasttext_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfull_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ss/lib/python3.7/site-packages/gensim/models/fasttext.py\u001b[0m in \u001b[0;36m_load_fasttext_format\u001b[0;34m(model_file, encoding, full_model)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m     \"\"\"\n\u001b[0;32m--> 807\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fasttext_bin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfull_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ss/lib/python3.7/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m     )\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ss/lib/python3.7/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cc.hi.300.bin'"
     ]
    }
   ],
   "source": [
    "#This piece of code loads word vector from fasttext .bin files and loads them into a text file.\n",
    "#For english it converts to lowercase before adding, only the first instance is added\n",
    "#It loads a subset of all words as governed by words_file variable\n",
    "#In this repo, i've already added txt file so this need not be run. \n",
    "#However, if you wish to add new languages from fastText, move bin files here and run this first.\n",
    "hi = True\n",
    "fasttext_file = \"cc.hi.300.bin\" if hi else \"cc.en.300.bin\"\n",
    "words_file = \"hi_words_reduced\" if hi else \"eng_words_reduced\"\n",
    "op_wv_file = \"ft.hi.300.txt\" if hi else \"ft.en.300.txt\"\n",
    "wv = fasttext.load_facebook_vectors(fasttext_file)\n",
    "\n",
    "file_length = sum(1 for i in open(words_file,'rb'))\n",
    "done_words = set()\n",
    "with open(words_file) as file:\n",
    "    with open(op_wv_file,\"w\") as wv_file:\n",
    "        for line in tqdm(file,total=file_length):\n",
    "            word = line.strip()\n",
    "            if not hi:\n",
    "                if word.lower() in done_words:\n",
    "                    continue\n",
    "                done_words.add(word.lower())\n",
    "            text = np.array2string(wv[word],max_line_width=9999999,formatter={'float':lambda x: \"%.5f\" % x})[1:-1]\n",
    "            #text = re.sub(\"\\s\\s+\",\" \",text)\n",
    "            text = word.lower()+\" \"+text\n",
    "            wv_file.write(text+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/babylonhealth/fastText_multilingual/blob/master/align_your_own.ipynb\n",
    "# from https://stackoverflow.com/questions/21030391/how-to-normalize-array-numpy\n",
    "def normalized(a, axis=-1, order=2):\n",
    "    \"\"\"Utility function to normalize the rows of a numpy array.\"\"\"\n",
    "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
    "    l2[l2==0] = 1\n",
    "    return a / np.expand_dims(l2, axis)\n",
    "\n",
    "def make_training_matrices(source_dictionary, target_dictionary, bilingual_dictionary):\n",
    "    \"\"\"\n",
    "    Source and target dictionaries are the FastVector objects of\n",
    "    source/target languages. bilingual_dictionary is a list of \n",
    "    translation pair tuples [(source_word, target_word), ...].\n",
    "    \"\"\"\n",
    "    source_matrix = []\n",
    "    target_matrix = []\n",
    "\n",
    "    for (source, target) in bilingual_dictionary:\n",
    "        \n",
    "        try:\n",
    "            assert source in source_dictionary\n",
    "            assert target in target_dictionary\n",
    "        except AssertionError:\n",
    "            if (source not in source_dictionary) and (target not in target_dictionary):\n",
    "                print (\"Warning : Couplet not found - \",source,target)\n",
    "            elif target not in target_dictionary:\n",
    "                print (\"Warning : Target not found - \",target)\n",
    "            else:\n",
    "                print (\"Warning : Source not found - \",source)\n",
    "        source_matrix.append(source_dictionary[source])\n",
    "        target_matrix.append(target_dictionary[target])\n",
    "\n",
    "    # return training matrices\n",
    "    return np.array(source_matrix), np.array(target_matrix)\n",
    "\n",
    "def learn_transformation(source_matrix, target_matrix, normalize_vectors=True):\n",
    "    \"\"\"\n",
    "    Source and target matrices are numpy arrays, shape\n",
    "    (dictionary_length, embedding_dimension). These contain paired\n",
    "    word vectors from the bilingual dictionary.\n",
    "    \"\"\"\n",
    "    # optionally normalize the training vectors\n",
    "    if normalize_vectors:\n",
    "        source_matrix = normalized(source_matrix)\n",
    "        target_matrix = normalized(target_matrix)\n",
    "\n",
    "    # perform the SVD\n",
    "    product = np.matmul(source_matrix.transpose(), target_matrix)\n",
    "    U, s, V = np.linalg.svd(product)\n",
    "\n",
    "    # return orthogonal transformation which aligns source language to the target\n",
    "    return np.matmul(U, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 233556/233556 [00:39<00:00, 5930.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Loaded embedding file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 1164060/1164060 [03:57<00:00, 4896.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Loaded embedding file.\n"
     ]
    }
   ],
   "source": [
    "eng_glove = glove(\"ft.en.300.txt\",validate=False)\n",
    "hin_glove = glove(\"ft.hi.300.txt\",validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0841448]]\n",
      "[[0.00244874]]\n",
      "[[-0.06798166]]\n",
      "Not in bilingual dict -  [[-0.11828031]]\n",
      "Not in bilingual dict -  [[-0.03326376]]\n",
      "Not in bilingual dict -  [[0.09681229]]\n",
      "Not in bilingual dict -  [[-0.02088189]]\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity(eng_glove[\"hyderabad\"].reshape(1,-1),hin_glove[\"हैदराबाद\"].reshape(1,-1)))\n",
    "print(cosine_similarity(eng_glove[\"pink\"].reshape(1,-1),hin_glove[\"गुलाबी\"].reshape(1,-1)))\n",
    "print(cosine_similarity(eng_glove[\"thief\"].reshape(1,-1),hin_glove[\"चोर\"].reshape(1,-1)))\n",
    "print(\"Not in bilingual dict - \",cosine_similarity(eng_glove[\"clothes\"].reshape(1,-1),hin_glove[\"वस्त्र\"].reshape(1,-1)))\n",
    "print(\"Not in bilingual dict - \",cosine_similarity(eng_glove[\"wolf\"].reshape(1,-1),hin_glove[\"भेड़िया\"].reshape(1,-1)))\n",
    "print(\"Not in bilingual dict - \",cosine_similarity(eng_glove[\"cricket\"].reshape(1,-1),hin_glove[\"क्रिकेट\"].reshape(1,-1)))\n",
    "print(\"Not in bilingual dict - \",cosine_similarity(eng_glove[\"israel\"].reshape(1,-1),hin_glove[\"इज़राइल\"].reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilingual_dictionary = []\n",
    "\n",
    "with open(\"hin_eng_map.csv\") as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        bilingual_dictionary.append((line.split(\",\")[1],line.split(\",\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_matrix, target_matrix = make_training_matrices(eng_glove, hin_glove, bilingual_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = learn_transformation(source_matrix, target_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_glove.apply_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.57747114]]\n",
      "[[0.60449957]]\n",
      "[[0.66178923]]\n",
      "Not in bilingual dict -  [[0.50342158]]\n",
      "Not in bilingual dict -  [[0.3396294]]\n",
      "Not in bilingual dict -  [[0.54517332]]\n",
      "Not in bilingual dict -  [[0.45440497]]\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity(eng_glove[\"hyderabad\"].reshape(1,-1),hin_glove[\"हैदराबाद\"].reshape(1,-1)))\n",
    "print(cosine_similarity(eng_glove[\"pink\"].reshape(1,-1),hin_glove[\"गुलाबी\"].reshape(1,-1)))\n",
    "print(cosine_similarity(eng_glove[\"thief\"].reshape(1,-1),hin_glove[\"चोर\"].reshape(1,-1)))\n",
    "print(\"Not in bilingual dict - \",cosine_similarity(eng_glove[\"clothes\"].reshape(1,-1),hin_glove[\"वस्त्र\"].reshape(1,-1)))\n",
    "print(\"Not in bilingual dict - \",cosine_similarity(eng_glove[\"wolf\"].reshape(1,-1),hin_glove[\"भेड़िया\"].reshape(1,-1)))\n",
    "print(\"Not in bilingual dict - \",cosine_similarity(eng_glove[\"cricket\"].reshape(1,-1),hin_glove[\"क्रिकेट\"].reshape(1,-1)))\n",
    "print(\"Not in bilingual dict - \",cosine_similarity(eng_glove[\"israel\"].reshape(1,-1),hin_glove[\"इज़राइल\"].reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1164060/1164060 [08:39<00:00, 2242.56it/s]\n"
     ]
    }
   ],
   "source": [
    "hin_glove.save_to_file(\"ft.hi_aligned_en.300.txt\")\n",
    "with open(\"ft.hi_aligned_en.300.txt\",\"a\") as file:\n",
    "    with open(eng_glove.glove_path,\"r\") as add_file:\n",
    "        for line in add_file:\n",
    "            file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1397616/1397616 [02:32<00:00, 9178.00it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Loaded embedding file.\n"
     ]
    }
   ],
   "source": [
    "eng_hin_glove = glove(\"ft.hi_aligned_en.300.txt\",validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fukuoka', 0.6802664000656203),\n",
       " ('hokkaido', 0.6724470995947466),\n",
       " ('okinawa', 0.6690219963532618),\n",
       " ('nagoya', 0.6675088791353242),\n",
       " ('shizuoka', 0.6603984128508753),\n",
       " ('taiwan', 0.6567059421467693),\n",
       " ('kansai', 0.6454585646024337),\n",
       " ('sendai', 0.6438484792647167),\n",
       " ('niigata', 0.6415781006073018),\n",
       " ('china', 0.6340063062782523),\n",
       " ('kumamoto', 0.6331457422374301),\n",
       " ('tohoku', 0.628250787466561),\n",
       " ('sapporo', 0.6180111953647428),\n",
       " ('chiba', 0.6121639072289121),\n",
       " ('toyama', 0.6114172864798127),\n",
       " ('aomori', 0.6102078969820739),\n",
       " ('kagoshima', 0.6091106071919026),\n",
       " ('kanagawa', 0.6058339761429379),\n",
       " ('ibaraki', 0.6013813756276624),\n",
       " ('nagasaki', 0.6012000186819042)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_hin_glove.most_similar(\"japan\",20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46020392017853806"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_hin_glove.similarity(\"प्रचुर\",\"plentiful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17477954981472577"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_hin_glove.similarity(\"सूखा\",\"famine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
