{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import fasttext\n",
    "from get_glove_embeddings import glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#This piece of code loads word vector from fasttext .bin files and loads them into a text file.\n",
    "#For english it converts to lowercase before adding, only the first instance is added\n",
    "#It loads a subset of all words as governed by words_file variable\n",
    "#In this repo, i've already added txt file so this need not be run. \n",
    "#However, if you wish to add new languages from fastText, move bin files here and run this first.\n",
    "# hi = True\n",
    "# fasttext_file = \"cc.hi.300.bin\" if hi else \"cc.en.300.bin\"\n",
    "# words_file = \"hi_words_reduced\" if hi else \"eng_words_reduced\"\n",
    "# op_wv_file = \"ft.hi.300.txt\" if hi else \"ft.en.300.txt\"\n",
    "# wv = fasttext.load_facebook_vectors(fasttext_file)\n",
    "\n",
    "# file_length = sum(1 for i in open(words_file,'rb'))\n",
    "# done_words = set()\n",
    "# with open(words_file) as file:\n",
    "#     with open(op_wv_file,\"w\") as wv_file:\n",
    "#         for line in tqdm(file,total=file_length):\n",
    "#             word = line.strip()\n",
    "#             if not hi:\n",
    "#                 if word.lower() in done_words:\n",
    "#                     continue\n",
    "#                 done_words.add(word.lower())\n",
    "#             text = np.array2string(wv[word],max_line_width=9999999,formatter={'float':lambda x: \"%.5f\" % x})[1:-1]\n",
    "#             #text = re.sub(\"\\s\\s+\",\" \",text)\n",
    "#             text = word.lower()+\" \"+text\n",
    "#             wv_file.write(text+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/babylonhealth/fastText_multilingual/blob/master/align_your_own.ipynb\n",
    "# from https://stackoverflow.com/questions/21030391/how-to-normalize-array-numpy\n",
    "def normalized(a, axis=-1, order=2):\n",
    "    \"\"\"Utility function to normalize the rows of a numpy array.\"\"\"\n",
    "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
    "    l2[l2==0] = 1\n",
    "    return a / np.expand_dims(l2, axis)\n",
    "\n",
    "def make_training_matrices(source_dictionary, target_dictionary, bilingual_dictionary):\n",
    "    \"\"\"\n",
    "    Source and target dictionaries are the FastVector objects of\n",
    "    source/target languages. bilingual_dictionary is a list of \n",
    "    translation pair tuples [(source_word, target_word), ...].\n",
    "    \"\"\"\n",
    "    source_matrix = []\n",
    "    target_matrix = []\n",
    "\n",
    "    for (source, target) in bilingual_dictionary:\n",
    "        \n",
    "        try:\n",
    "            assert source in source_dictionary\n",
    "            assert target in target_dictionary\n",
    "        except AssertionError:\n",
    "            if (source not in source_dictionary) and (target not in target_dictionary):\n",
    "                print (\"Warning : Couplet not found - \",source,target)\n",
    "            elif target not in target_dictionary:\n",
    "                print (\"Warning : Target not found - \",target)\n",
    "            else:\n",
    "                print (\"Warning : Source not found - \",source)\n",
    "        source_matrix.append(source_dictionary[source])\n",
    "        target_matrix.append(target_dictionary[target])\n",
    "\n",
    "    # return training matrices\n",
    "    return np.array(source_matrix), np.array(target_matrix)\n",
    "\n",
    "def learn_transformation(source_matrix, target_matrix, normalize_vectors=True):\n",
    "    \"\"\"\n",
    "    Source and target matrices are numpy arrays, shape\n",
    "    (dictionary_length, embedding_dimension). These contain paired\n",
    "    word vectors from the bilingual dictionary.\n",
    "    \"\"\"\n",
    "    # optionally normalize the training vectors\n",
    "    if normalize_vectors:\n",
    "        source_matrix = normalized(source_matrix)\n",
    "        target_matrix = normalized(target_matrix)\n",
    "\n",
    "    # perform the SVD\n",
    "    product = np.matmul(source_matrix.transpose(), target_matrix)\n",
    "    U, s, V = np.linalg.svd(product)\n",
    "\n",
    "    # return orthogonal transformation which aligns source language to the target\n",
    "    return np.matmul(U, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 1164060/1164060 [03:01<00:00, 6425.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Loaded embedding file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 233556/233556 [00:33<00:00, 6903.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Loaded embedding file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hin_glove = glove(\"ft.hi.300.txt\",validate=False)\n",
    "eng_glove = glove(\"ft.en.300.txt\",validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0841448]]\n",
      "[[0.00244874]]\n",
      "[[-0.06798166]]\n",
      "Not in bilingual dict -  [[-0.11828031]]\n",
      "Not in bilingual dict -  [[-0.03326376]]\n",
      "Not in bilingual dict -  [[0.09681229]]\n",
      "Not in bilingual dict -  [[-0.02088189]]\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity(eng_glove[\"hyderabad\"].reshape(1,-1),hin_glove[\"हैदराबाद\"].reshape(1,-1)))\n",
    "print(cosine_similarity(eng_glove[\"pink\"].reshape(1,-1),hin_glove[\"गुलाबी\"].reshape(1,-1)))\n",
    "print(cosine_similarity(eng_glove[\"thief\"].reshape(1,-1),hin_glove[\"चोर\"].reshape(1,-1)))\n",
    "print(\"Not in bilingual dict - \",cosine_similarity(eng_glove[\"clothes\"].reshape(1,-1),hin_glove[\"वस्त्र\"].reshape(1,-1)))\n",
    "print(\"Not in bilingual dict - \",cosine_similarity(eng_glove[\"wolf\"].reshape(1,-1),hin_glove[\"भेड़िया\"].reshape(1,-1)))\n",
    "print(\"Not in bilingual dict - \",cosine_similarity(eng_glove[\"cricket\"].reshape(1,-1),hin_glove[\"क्रिकेट\"].reshape(1,-1)))\n",
    "print(\"Not in bilingual dict - \",cosine_similarity(eng_glove[\"israel\"].reshape(1,-1),hin_glove[\"इज़राइल\"].reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilingual_dictionary = []\n",
    "\n",
    "with open(\"hin_eng_map.csv\",encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        bilingual_dictionary.append((line.split(\",\")[1],line.split(\",\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_matrix, target_matrix = make_training_matrices(eng_glove, hin_glove, bilingual_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = learn_transformation(source_matrix, target_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_glove.apply_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5746449]]\n",
      "[[0.59595558]]\n",
      "[[0.66035739]]\n",
      "Not in bilingual dict -  [[0.49474255]]\n",
      "Not in bilingual dict -  [[0.35297204]]\n",
      "Not in bilingual dict -  [[0.54519973]]\n",
      "Not in bilingual dict -  [[0.44262905]]\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity(eng_glove[\"hyderabad\"].reshape(1,-1),hin_glove[\"हैदराबाद\"].reshape(1,-1)))\n",
    "print(cosine_similarity(eng_glove[\"pink\"].reshape(1,-1),hin_glove[\"गुलाबी\"].reshape(1,-1)))\n",
    "print(cosine_similarity(eng_glove[\"thief\"].reshape(1,-1),hin_glove[\"चोर\"].reshape(1,-1)))\n",
    "print(\"Not in bilingual dict - \",cosine_similarity(eng_glove[\"clothes\"].reshape(1,-1),hin_glove[\"वस्त्र\"].reshape(1,-1)))\n",
    "print(\"Not in bilingual dict - \",cosine_similarity(eng_glove[\"wolf\"].reshape(1,-1),hin_glove[\"भेड़िया\"].reshape(1,-1)))\n",
    "print(\"Not in bilingual dict - \",cosine_similarity(eng_glove[\"cricket\"].reshape(1,-1),hin_glove[\"क्रिकेट\"].reshape(1,-1)))\n",
    "print(\"Not in bilingual dict - \",cosine_similarity(eng_glove[\"israel\"].reshape(1,-1),hin_glove[\"इज़राइल\"].reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 233556/233556 [03:01<00:00, 1287.32it/s]\n"
     ]
    }
   ],
   "source": [
    "eng_glove.save_to_file(\"ft.hi_aligned_en.300.txt\")\n",
    "with open(\"ft.hi_aligned_en.300.txt\",\"a\",encoding=\"utf-8\") as file:\n",
    "    with open(hin_glove.glove_path,\"r\",encoding='utf-8') as add_file:\n",
    "        for line in add_file:\n",
    "            file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 1397616/1397616 [03:44<00:00, 6238.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Loaded embedding file.\n"
     ]
    }
   ],
   "source": [
    "eng_hin_glove = glove(\"ft.hi_aligned_en.300.txt\",validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('straightforward', 0.7797719371890728),\n",
       " ('सरल', 0.6859045112119004),\n",
       " ('simplistic', 0.6471223587865472),\n",
       " ('uncomplicated', 0.6373788345529645),\n",
       " ('elegant', 0.5848393424989479),\n",
       " ('plain', 0.5418345928929682),\n",
       " ('आसान', 0.5410338990547959),\n",
       " ('simplicity', 0.5400507554476914),\n",
       " ('clever', 0.5178662900313972),\n",
       " ('ingenious', 0.5139553814262425)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_hin_glove.most_similar(\"simple\",10) #We see here आसान which was not in the dictionary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
